{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ollama Colab demo\n",
        "\n",
        "Version 2: in-thread\n",
        "\n",
        "## Step 1: runtime\n",
        "\n",
        "Ensure your Colab runtime is \"T4 GPU\" through the _Runtime_ => _Change Runtime_ menu.\n",
        "\n",
        "After that, execute the next cells."
      ],
      "metadata": {
        "id": "1mq5AJySJU3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: install pciutils and ollama\n",
        "\n",
        "The warning about systemd should not be a blocker."
      ],
      "metadata": {
        "id": "rMGZyAl8Jiya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!sudo apt install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "skfQjoH5Jhlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: start ollama in a thread, from Python\n",
        "\n",
        "You can launch a long-running process right from within Python and leave it running while you move on to the next cells:"
      ],
      "metadata": {
        "id": "fGEJwjTPoKWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama_serve():\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "thread = threading.Thread(target=run_ollama_serve)\n",
        "thread.start()\n",
        "time.sleep(5)"
      ],
      "metadata": {
        "id": "Jh5CBAFxBYAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: launch ollama commands\n",
        "\n",
        "Even with ollama started from Python, regular bang-commands reach the listening server and work:"
      ],
      "metadata": {
        "id": "WcBLqZfyoHg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama3.2"
      ],
      "metadata": {
        "id": "o2ghppmRDFny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: use ollama from Python\n",
        "\n",
        "A little demonstration (one of the possible ways):"
      ],
      "metadata": {
        "id": "TYQJNeTuni_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-ollama"
      ],
      "metadata": {
        "id": "MbrT39oil6tK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer as concisely as possible.\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "model = OllamaLLM(model=\"llama3.2\")\n",
        "\n",
        "chain = prompt | model"
      ],
      "metadata": {
        "id": "9quBP56zDvpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"question\": \"What are the evolutionary reasons for the dikaryophase in higher Eumycota?\"})"
      ],
      "metadata": {
        "id": "GFNMkWwirA4D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}