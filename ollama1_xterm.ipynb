{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ollama Colab demo\n",
        "\n",
        "Version 1: using `xterm`\n",
        "\n",
        "## Step 1: runtime\n",
        "\n",
        "Ensure your Colab runtime is \"T4 GPU\" through the _Runtime_ => _Change Runtime_ menu.\n",
        "\n",
        "After that, execute the next cells."
      ],
      "metadata": {
        "id": "xhSTUs1pzOA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: install xterm\n",
        "\n",
        "Run the following to install and load the xterm extension for Colab:"
      ],
      "metadata": {
        "id": "nSvhmfIfztVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q colab-xterm\n",
        "%load_ext colabxterm"
      ],
      "metadata": {
        "id": "Vw90u8YZ1FeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: start xterm\n",
        "\n",
        "This will remain open and provide a terminal into the container running the Colab. You will type commands in this window\n",
        "\n",
        "1. Be aware of a substantial typing latency and be patient when entering commands\n",
        "2. Do not mind the mingled formatting for progress bars, etc."
      ],
      "metadata": {
        "id": "oHQ-z9P8z_-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%xterm"
      ],
      "metadata": {
        "id": "YvVJGqQF1KU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: install and set up ollama\n",
        "\n",
        "Bring the focus into the xterm window above and, without fretting, type the following three commands. Wait for each one to complete before launching the next one.\n",
        "\n",
        "```\n",
        "# Command 1 to install ollama:\n",
        "curl https://ollama.ai/install.sh | sh\n",
        "\n",
        "# Command 2 to start ollama (in background. Type Enter after a little while to regain the xterm prompt):\n",
        "ollama serve &\n",
        "\n",
        "# Command 3 to retrieve a LLM:\n",
        "ollama pull llama3.2\n",
        "\n",
        "# You should see \"success\" on the last line of output by ollama.\n",
        "```"
      ],
      "metadata": {
        "id": "EIPTDgGq0Xz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: use ollama from other cells\n",
        "\n",
        "You can launch ollama commands with the usual notebook bang-notation:"
      ],
      "metadata": {
        "id": "Qlzyy5Ag2Tcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "id": "UI48iCSr2MaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to control ollama from within Python code you need e.g. this package (this is for Langchain)"
      ],
      "metadata": {
        "id": "Li9O8MwI2bUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q -U langchain-ollama"
      ],
      "metadata": {
        "id": "VQjcSQZl2N7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A small example to demonstrate. First a little setup:"
      ],
      "metadata": {
        "id": "_ja9QoD32jAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer as concisely as possible.\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "model = OllamaLLM(model=\"llama3.2\")\n",
        "\n",
        "chain = prompt | model"
      ],
      "metadata": {
        "id": "HoTW9iTH2X-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can run the model:"
      ],
      "metadata": {
        "id": "kJnTUk6B2qYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"question\": \"Are the anamorphs of Ascomycota all mapped to their holomorphs as of today? And if not, what would be the best technique to achieve that?\"})"
      ],
      "metadata": {
        "id": "L0YeD0nw2wMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Behind the scenes, what happens is localhost HTTP requests with ollama. Observe:\n",
        "\n",
        "_(note: this request turns off streaming responses for demonstration purposes, as outlined in [the docs](https://github.com/ollama/ollama/blob/main/docs/api.md#request-no-streaming).)_"
      ],
      "metadata": {
        "id": "RBbGEJhj4tJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "body = {\n",
        "    \"model\": \"llama3.2\",\n",
        "    \"prompt\": \"Is Puccinia graminis an obligate parasite of Poaceae?\",\n",
        "    \"stream\": False,\n",
        "}\n",
        "\n",
        "response = requests.post(\"http://localhost:11434/api/generate\", json=body)\n",
        "resp_json = response.json()\n",
        "\n",
        "resp_short = {k: f\"{str(v)[:30]} ...\" for k, v in resp_json.items()}\n",
        "print(\"Response (shortened):\")\n",
        "print(json.dumps(resp_short, indent=2))\n",
        "\n",
        "print(\"\\nAnswer:\")\n",
        "print(resp_json[\"response\"])"
      ],
      "metadata": {
        "id": "pwcm_STd3Ndd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check that each request is being logged in the xterm panel above."
      ],
      "metadata": {
        "id": "W6fwOf8549mj"
      }
    }
  ]
}